{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import numpy\n",
    "MNIST = input_data.read_data_sets(\"./MNIST_data\", one_hot = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 loss =  2.425958\n",
      "Epoch: 02 loss =  2.416129\n",
      "Epoch: 03 loss =  2.407149\n",
      "Epoch: 04 loss =  2.398926\n",
      "Epoch: 05 loss =  2.391381\n",
      "Epoch: 06 loss =  2.384446\n",
      "Epoch: 07 loss =  2.378061\n",
      "Epoch: 08 loss =  2.372173\n",
      "Epoch: 09 loss =  2.366739\n",
      "Epoch: 10 loss =  2.361721\n",
      "Epoch: 11 loss =  2.357083\n",
      "Epoch: 12 loss =  2.352791\n",
      "Epoch: 13 loss =  2.348817\n",
      "Epoch: 14 loss =  2.345137\n",
      "Epoch: 15 loss =  2.341728\n",
      "Test accuracy: 0.0958\n",
      "Epoch: 01 loss =  2.857259\n",
      "Epoch: 02 loss =  2.829148\n",
      "Epoch: 03 loss =  2.802808\n",
      "Epoch: 04 loss =  2.779270\n",
      "Epoch: 05 loss =  2.753926\n",
      "Epoch: 06 loss =  2.732126\n",
      "Epoch: 07 loss =  2.711036\n",
      "Epoch: 08 loss =  2.691214\n",
      "Epoch: 09 loss =  2.672051\n",
      "Epoch: 10 loss =  2.654346\n",
      "Epoch: 11 loss =  2.636300\n",
      "Epoch: 12 loss =  2.619442\n",
      "Epoch: 13 loss =  2.605752\n",
      "Epoch: 14 loss =  2.589259\n",
      "Epoch: 15 loss =  2.575592\n",
      "Test accuracy: 0.0892\n",
      "Epoch: 01 loss =  2.386607\n",
      "Epoch: 02 loss =  2.352633\n",
      "Epoch: 03 loss =  2.331308\n",
      "Epoch: 04 loss =  2.319316\n",
      "Epoch: 05 loss =  2.311857\n",
      "Epoch: 06 loss =  2.307702\n",
      "Epoch: 07 loss =  2.304733\n",
      "Epoch: 08 loss =  2.303530\n",
      "Epoch: 09 loss =  2.302611\n",
      "Epoch: 10 loss =  2.301897\n",
      "Epoch: 11 loss =  2.301669\n",
      "Epoch: 12 loss =  2.301403\n",
      "Epoch: 13 loss =  2.301402\n",
      "Epoch: 14 loss =  2.301349\n",
      "Epoch: 15 loss =  2.301213\n",
      "Test accuracy: 0.1135\n",
      "Epoch: 01 loss =  2.555438\n",
      "Epoch: 02 loss =  2.455888\n",
      "Epoch: 03 loss =  2.398443\n",
      "Epoch: 04 loss =  2.361213\n",
      "Epoch: 05 loss =  2.339751\n",
      "Epoch: 06 loss =  2.325042\n",
      "Epoch: 07 loss =  2.316337\n",
      "Epoch: 08 loss =  2.310336\n",
      "Epoch: 09 loss =  2.307254\n",
      "Epoch: 10 loss =  2.304832\n",
      "Epoch: 11 loss =  2.303483\n",
      "Epoch: 12 loss =  2.302538\n",
      "Epoch: 13 loss =  2.302142\n",
      "Epoch: 14 loss =  2.301731\n",
      "Epoch: 15 loss =  2.301428\n",
      "Epoch: 16 loss =  2.301558\n",
      "Epoch: 17 loss =  2.301148\n",
      "Epoch: 18 loss =  2.301365\n",
      "Epoch: 19 loss =  2.301146\n",
      "Epoch: 20 loss =  2.301349\n",
      "Test accuracy: 0.1135\n",
      "Epoch: 01 loss =  0.278805\n",
      "Epoch: 02 loss =  0.096657\n",
      "Epoch: 03 loss =  0.062492\n",
      "Epoch: 04 loss =  0.043737\n",
      "Epoch: 05 loss =  0.032768\n",
      "Epoch: 06 loss =  0.025197\n",
      "Epoch: 07 loss =  0.020157\n",
      "Epoch: 08 loss =  0.017373\n",
      "Epoch: 09 loss =  0.017248\n",
      "Epoch: 10 loss =  0.012584\n",
      "Epoch: 11 loss =  0.011087\n",
      "Epoch: 12 loss =  0.012247\n",
      "Epoch: 13 loss =  0.010656\n",
      "Epoch: 14 loss =  0.011601\n",
      "Epoch: 15 loss =  0.009320\n",
      "Test accuracy: 0.9811\n",
      "Epoch: 01 loss =  2.041288\n",
      "Epoch: 02 loss =  1.590630\n",
      "Epoch: 03 loss =  1.241443\n",
      "Epoch: 04 loss =  0.996269\n",
      "Epoch: 05 loss =  0.835042\n",
      "Epoch: 06 loss =  0.725768\n",
      "Epoch: 07 loss =  0.651618\n",
      "Epoch: 08 loss =  0.600016\n",
      "Epoch: 09 loss =  0.558132\n",
      "Epoch: 10 loss =  0.526166\n",
      "Epoch: 11 loss =  0.501258\n",
      "Epoch: 12 loss =  0.479363\n",
      "Epoch: 13 loss =  0.462702\n",
      "Epoch: 14 loss =  0.447785\n",
      "Epoch: 15 loss =  0.434633\n",
      "Test accuracy: 0.8912\n",
      "Epoch: 01 loss =  0.478101\n",
      "Epoch: 02 loss =  0.206900\n",
      "Epoch: 03 loss =  0.147138\n",
      "Epoch: 04 loss =  0.110268\n",
      "Epoch: 05 loss =  0.085843\n",
      "Epoch: 06 loss =  0.066030\n",
      "Epoch: 07 loss =  0.051934\n",
      "Epoch: 08 loss =  0.041945\n",
      "Epoch: 09 loss =  0.032070\n",
      "Epoch: 10 loss =  0.025259\n",
      "Epoch: 11 loss =  0.022214\n",
      "Epoch: 12 loss =  0.016335\n",
      "Epoch: 13 loss =  0.012467\n",
      "Epoch: 14 loss =  0.012196\n",
      "Epoch: 15 loss =  0.010222\n",
      "Test accuracy: 0.9789\n",
      "Epoch: 01 loss =  2.333417\n",
      "Epoch: 02 loss =  2.293753\n",
      "Epoch: 03 loss =  2.288911\n",
      "Epoch: 04 loss =  2.284117\n",
      "Epoch: 05 loss =  2.279241\n",
      "Epoch: 06 loss =  2.274355\n",
      "Epoch: 07 loss =  2.269474\n",
      "Epoch: 08 loss =  2.264458\n",
      "Epoch: 09 loss =  2.259239\n",
      "Epoch: 10 loss =  2.253825\n",
      "Epoch: 11 loss =  2.248588\n",
      "Epoch: 12 loss =  2.243200\n",
      "Epoch: 13 loss =  2.237351\n",
      "Epoch: 14 loss =  2.231614\n",
      "Epoch: 15 loss =  2.225450\n",
      "Test accuracy: 0.3545\n",
      "Epoch: 01 loss =  0.269835\n",
      "Epoch: 02 loss =  0.095257\n",
      "Epoch: 03 loss =  0.061237\n",
      "Epoch: 04 loss =  0.042927\n",
      "Epoch: 05 loss =  0.033283\n",
      "Epoch: 06 loss =  0.024349\n",
      "Epoch: 07 loss =  0.020518\n",
      "Epoch: 08 loss =  0.018480\n",
      "Epoch: 09 loss =  0.015193\n",
      "Epoch: 10 loss =  0.014928\n",
      "Epoch: 11 loss =  0.011443\n",
      "Epoch: 12 loss =  0.011401\n",
      "Epoch: 13 loss =  0.010473\n",
      "Epoch: 14 loss =  0.009458\n",
      "Epoch: 15 loss =  0.009743\n",
      "Test accuracy: 0.9796\n",
      "Epoch: 01 loss =  0.259132\n",
      "Epoch: 02 loss =  0.083421\n",
      "Epoch: 03 loss =  0.051216\n",
      "Epoch: 04 loss =  0.033991\n",
      "Epoch: 05 loss =  0.022726\n",
      "Epoch: 06 loss =  0.018796\n",
      "Epoch: 07 loss =  0.014921\n",
      "Epoch: 08 loss =  0.012983\n",
      "Epoch: 09 loss =  0.011681\n",
      "Epoch: 10 loss =  0.006314\n",
      "Epoch: 11 loss =  0.015331\n",
      "Epoch: 12 loss =  0.006936\n",
      "Epoch: 13 loss =  0.008178\n",
      "Epoch: 14 loss =  0.008527\n",
      "Epoch: 15 loss =  0.009324\n",
      "Test accuracy: 0.9783\n"
     ]
    }
   ],
   "source": [
    "def get_train_batch_func(batch_size, fake_data = False):\n",
    "    #\"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "    self = MNIST.train\n",
    "    if fake_data:\n",
    "        fake_image = [1] * 784\n",
    "        if self.one_hot:\n",
    "            fake_label = [1] + [0] * 9\n",
    "        else:\n",
    "            fake_label = 0\n",
    "        return [fake_image for _ in xrange(batch_size)], [\n",
    "            fake_label for _ in xrange(batch_size)\n",
    "        ]\n",
    "    start = self._index_in_epoch\n",
    "    self._index_in_epoch += 128       #원래는 batch_size였음\n",
    "    if self._index_in_epoch > self._num_examples:\n",
    "        \n",
    "        if self._index_in_epoch - self._num_examples < 128: #마지막으로 N % 128 개인 경우\n",
    "            self._index_in_epoch = self._num_examples \n",
    "            \n",
    "        else:\n",
    "            # Finished epoch\n",
    "            self._epochs_completed += 1\n",
    "            # Shuffle the data\n",
    "            perm = numpy.arange(self._num_examples)\n",
    "            numpy.random.shuffle(perm)\n",
    "            self._images = self._images[perm]\n",
    "            self._labels = self._labels[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = 128  #원래는 batch_size 였음\n",
    "            assert batch_size <= self._num_examples\n",
    "    \n",
    "    end = self._index_in_epoch\n",
    "    return self._images[start:end], self._labels[start:end]\n",
    "    \n",
    "\n",
    "\n",
    "def exec1(l_rate, t_epochs, b_size, directory_name, act_func, optimizer_func, get_train_batch):\n",
    "    #############################exercise2 : double layered fully connected neural network with softmax classifier###########################\n",
    "    #parameters ########exercise2-1: should change parameters and compare results\n",
    "    learning_rate =l_rate #learning_rate = 0.001\n",
    "    training_epochs = t_epochs #training_epochs = 2\n",
    "    batch_size = b_size #batch_size = 100\n",
    "\n",
    "    SUMMARY_DIR = directory_name\n",
    "    \n",
    "    #load mnist dataset\n",
    "    \n",
    "    with tf.name_scope('input') as scope:\n",
    "        X = tf.placeholder(tf.float32, [None, 784], name = 'image')\n",
    "        y = tf.placeholder(tf.float32, [None, 10], name = 'label')    \n",
    "    \n",
    "    with tf.variable_scope('layer1') as scope:\n",
    "        W1 = tf.get_variable(\"W\", shape = [784, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b1 = tf.Variable(tf.random_normal([512]))\n",
    "        L1 = act_func(tf.add(tf.matmul(X, W1), b1))#########################################ex1-2act_func\n",
    "    \n",
    "        tf.summary.histogram(\"X\", X)\n",
    "        tf.summary.histogram(\"weights\", W1)\n",
    "        tf.summary.histogram(\"bias\", b1)\n",
    "        tf.summary.histogram(\"layer\", L1)    \n",
    "    \n",
    "    with tf.variable_scope('layer12') as scope:\n",
    "        W12 = tf.get_variable(\"W\", shape = [512, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b12 = tf.Variable(tf.random_normal([512]))\n",
    "        L12 = act_func(tf.add(tf.matmul(L1, W12), b12))#########################################ex1-2act_func\n",
    "    \n",
    "        tf.summary.histogram(\"weights\", W12)\n",
    "        tf.summary.histogram(\"bias\", b12)\n",
    "        tf.summary.histogram(\"layer\", L12)\n",
    "\n",
    "    with tf.variable_scope('layer2') as scope:\n",
    "        W2 = tf.get_variable(\"W\", shape = [512, 10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b2 = tf.Variable(tf.random_normal([10]))\n",
    "        y_ = tf.add(tf.matmul(L12, W2), b2)\n",
    "    \n",
    "        tf.summary.histogram(\"weights\", W2)\n",
    "        tf.summary.histogram(\"bias\", b2)\n",
    "        tf.summary.histogram(\"logits\", y_)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_, labels = y))\n",
    "    optimizer = optimizer_func(learning_rate = learning_rate).minimize(loss) #################################ex1-2optimizer\n",
    "\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    #merge all summaries\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "    global_step = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        writer = tf.summary.FileWriter(SUMMARY_DIR, sess.graph)\n",
    "        for epoch in range(training_epochs):\n",
    "            total_batch = int(MNIST.train.num_examples / batch_size)\n",
    "            avg_loss = 0\n",
    "        \n",
    "            for i in range(total_batch):\n",
    "                batch_xs, batch_ys = get_train_batch(batch_size)##########################exercise2-3 get_train_batch구현,적용\n",
    "                feed_dict = {X: batch_xs, y: batch_ys}\n",
    "                s, I, _ = sess.run([summary, loss, optimizer], feed_dict = feed_dict)\n",
    "                writer.add_summary(s, global_step = global_step)#무언가 문제가 있음 원래는 파라미터가 s, I, glbal_step이었는데 I 뺌 일단\n",
    "                global_step += 1\n",
    "                avg_loss += I\n",
    "            print('Epoch:', '%02d' % (epoch + 1), 'loss = ', '{:6f}'.format(avg_loss / total_batch))\n",
    "    \n",
    "        correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        acc = sess.run(accuracy, feed_dict = {X: MNIST.test.images, y: MNIST.test.labels})\n",
    "        print('Test accuracy:', acc)\n",
    "    \n",
    "    tf.reset_default_graph() ## layer 이름의 중복 문제 해결을 위해 추가.    \n",
    "#l_rate, t_epochs, b_size,\n",
    "#learning_rate = 0.001\n",
    "#training_epochs = 2\n",
    "#batch_size = 100\n",
    "dir_name0 = './mnist3'\n",
    "dir_name1 = './dir_of_log1'\n",
    "#############################exercise2\n",
    "exec1(0.001, 15, 100, dir_name0, tf.nn.softmax, tf.train.GradientDescentOptimizer, MNIST.train.next_batch) #exercise2-1\n",
    "exec1(0.001, 15, 150, dir_name0, tf.nn.softmax, tf.train.GradientDescentOptimizer, MNIST.train.next_batch) #exercise2-1-1\n",
    "exec1(0.005, 15, 100, dir_name0, tf.nn.softmax, tf.train.GradientDescentOptimizer, MNIST.train.next_batch) #exercise2-1-1\n",
    "exec1(0.005, 20, 100, dir_name0, tf.nn.softmax, tf.train.GradientDescentOptimizer, MNIST.train.next_batch) #exercise2-1-1\n",
    "\n",
    "#############################exercise2-1\n",
    "exec1(0.001, 15, 100, './dir_of_log01', tf.nn.relu, tf.train.AdamOptimizer, MNIST.train.next_batch) #exercise3-1-2\n",
    "exec1(0.001, 15, 100, './dir_of_log02', tf.nn.relu, tf.train.GradientDescentOptimizer, MNIST.train.next_batch) #exercise3-1-2\n",
    "exec1(0.001, 15, 100, './dir_of_log03', tf.nn.sigmoid, tf.train.AdamOptimizer, MNIST.train.next_batch) #exercise3-1-2\n",
    "exec1(0.001, 15, 100, './dir_of_log04', tf.nn.sigmoid, tf.train.GradientDescentOptimizer, MNIST.train.next_batch) #exercise3-1-2\n",
    "\n",
    "#############################exercise3\n",
    "exec1(0.001, 15, 100, './dir_of_log11', tf.nn.relu, tf.train.AdamOptimizer, MNIST.train.next_batch) #exercise3-1-2\n",
    "exec1(0.001, 15, 100, './dir_of_log12', tf.nn.relu, tf.train.AdamOptimizer, get_train_batch_func) #exercise3-2-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 loss =  0.275439\n",
      "Epoch: 02 loss =  0.093915\n",
      "Epoch: 03 loss =  0.061334\n",
      "Epoch: 04 loss =  0.042007\n",
      "Epoch: 05 loss =  0.028348\n",
      "Epoch: 06 loss =  0.024801\n",
      "Epoch: 07 loss =  0.019204\n",
      "Epoch: 08 loss =  0.018717\n",
      "Epoch: 09 loss =  0.015804\n",
      "Epoch: 10 loss =  0.013966\n",
      "Epoch: 11 loss =  0.011437\n",
      "Epoch: 12 loss =  0.012661\n",
      "Epoch: 13 loss =  0.010399\n",
      "Epoch: 14 loss =  0.008573\n",
      "Epoch: 15 loss =  0.008139\n",
      "Test accuracy: 0.9816\n",
      "Epoch: 01 loss =  0.303015\n",
      "Epoch: 02 loss =  0.107602\n",
      "Epoch: 03 loss =  0.067260\n",
      "Epoch: 04 loss =  0.048599\n",
      "Epoch: 05 loss =  0.034331\n",
      "Epoch: 06 loss =  0.024124\n",
      "Epoch: 07 loss =  0.018206\n",
      "Epoch: 08 loss =  0.017932\n",
      "Epoch: 09 loss =  0.013319\n",
      "Epoch: 10 loss =  0.012771\n",
      "Epoch: 11 loss =  0.014019\n",
      "Epoch: 12 loss =  0.010192\n",
      "Epoch: 13 loss =  0.013513\n",
      "Epoch: 14 loss =  0.006999\n",
      "Epoch: 15 loss =  0.004064\n",
      "Test accuracy: 0.9811\n",
      "Epoch: 01 loss =  0.270252\n",
      "Epoch: 02 loss =  0.109077\n",
      "Epoch: 03 loss =  0.083485\n",
      "Epoch: 04 loss =  0.069764\n",
      "Epoch: 05 loss =  0.068152\n",
      "Epoch: 06 loss =  0.065226\n",
      "Epoch: 07 loss =  0.054479\n",
      "Epoch: 08 loss =  0.053099\n",
      "Epoch: 09 loss =  0.049939\n",
      "Epoch: 10 loss =  0.041085\n",
      "Epoch: 11 loss =  0.042852\n",
      "Epoch: 12 loss =  0.041346\n",
      "Epoch: 13 loss =  0.037977\n",
      "Epoch: 14 loss =  0.041641\n",
      "Epoch: 15 loss =  0.037858\n",
      "Test accuracy: 0.9745\n",
      "Epoch: 01 loss =  0.266764\n",
      "Epoch: 02 loss =  0.113571\n",
      "Epoch: 03 loss =  0.077464\n",
      "Epoch: 04 loss =  0.069687\n",
      "Epoch: 05 loss =  0.060430\n",
      "Epoch: 06 loss =  0.061670\n",
      "Epoch: 07 loss =  0.052223\n",
      "Epoch: 08 loss =  0.053574\n",
      "Epoch: 09 loss =  0.043332\n",
      "Epoch: 10 loss =  0.038123\n",
      "Epoch: 11 loss =  0.038123\n",
      "Epoch: 12 loss =  0.045017\n",
      "Epoch: 13 loss =  0.036064\n",
      "Epoch: 14 loss =  0.035905\n",
      "Epoch: 15 loss =  0.036640\n",
      "Epoch: 16 loss =  0.034731\n",
      "Epoch: 17 loss =  0.032874\n",
      "Epoch: 18 loss =  0.031931\n",
      "Epoch: 19 loss =  0.031232\n",
      "Epoch: 20 loss =  0.033540\n",
      "Test accuracy: 0.9752\n"
     ]
    }
   ],
   "source": [
    "#Supplement of exercise2\n",
    "exec1(0.001, 15, 100, './dir_of_log20', tf.nn.relu, tf.train.AdamOptimizer, MNIST.train.next_batch) #exercise2-1\n",
    "exec1(0.001, 15, 150, './dir_of_log20', tf.nn.relu, tf.train.AdamOptimizer, MNIST.train.next_batch) #exercise2-1-1\n",
    "exec1(0.005, 15, 100, './dir_of_log20', tf.nn.relu, tf.train.AdamOptimizer, MNIST.train.next_batch) #exercise2-1-1\n",
    "exec1(0.005, 20, 100, './dir_of_log20', tf.nn.relu, tf.train.AdamOptimizer, MNIST.train.next_batch) #exercise2-1-1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 loss =  2.044894\n",
      "Epoch: 02 loss =  1.558917\n",
      "Epoch: 03 loss =  1.203438\n",
      "Epoch: 04 loss =  0.968119\n",
      "Epoch: 05 loss =  0.813403\n",
      "Epoch: 06 loss =  0.715878\n",
      "Epoch: 07 loss =  0.641918\n",
      "Epoch: 08 loss =  0.592449\n",
      "Epoch: 09 loss =  0.550843\n",
      "Epoch: 10 loss =  0.519210\n",
      "Epoch: 11 loss =  0.494973\n",
      "Epoch: 12 loss =  0.475159\n",
      "Epoch: 13 loss =  0.457494\n",
      "Epoch: 14 loss =  0.443039\n",
      "Epoch: 15 loss =  0.428721\n",
      "Test accuracy: 0.888\n",
      "Epoch: 01 loss =  2.080448\n",
      "Epoch: 02 loss =  1.603906\n",
      "Epoch: 03 loss =  1.254522\n",
      "Epoch: 04 loss =  1.010791\n",
      "Epoch: 05 loss =  0.846787\n",
      "Epoch: 06 loss =  0.741750\n",
      "Epoch: 07 loss =  0.663419\n",
      "Epoch: 08 loss =  0.607991\n",
      "Epoch: 09 loss =  0.570051\n",
      "Epoch: 10 loss =  0.532505\n",
      "Epoch: 11 loss =  0.507253\n",
      "Epoch: 12 loss =  0.485610\n",
      "Epoch: 13 loss =  0.466803\n",
      "Epoch: 14 loss =  0.451988\n",
      "Epoch: 15 loss =  0.437407\n",
      "Test accuracy: 0.8906\n"
     ]
    }
   ],
   "source": [
    "#Supplement of exercise3\n",
    "exec1(0.001, 15, 100, './dir_of_log11', tf.nn.relu, tf.train.GradientDescentOptimizer, MNIST.train.next_batch) #exercise3-1-2\n",
    "exec1(0.001, 15, 100, './dir_of_log12', tf.nn.relu, tf.train.GradientDescentOptimizer, get_train_batch_func) #exercise3-2-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
